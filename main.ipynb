{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73174be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Directories of interest\n",
    "# include folders that you might want the llm to have access to\n",
    "DIRECTORIES = [r\"C:\\\\Users\\\\Jamin Carter\\\\Downloads\\\\web_archive\", r\"D:\\Project\\Web-scraping\\TestFolder\"]\n",
    "    \n",
    "    \n",
    "files_to_vectorize=[]\n",
    "for dir in DIRECTORIES:\n",
    "    files = Path(dir).glob(\"*\")\n",
    "    for file_path in files:\n",
    "        if file_path.is_file():\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                files_to_vectorize.append(str(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca78a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to vectorize: 73\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total files to vectorize: {len(files_to_vectorize)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593e0466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_game8_co_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_vectorize[7][55:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6934749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORY SETUP\n",
    "import os\n",
    "if not os.path.exists(\"./db/sqlite\"):\n",
    "    os.makedirs(\"./db/sqlite\")\n",
    "    \n",
    "\n",
    "if not os.path.exists(\"./db/chroma\"):\n",
    "    os.makedirs(\"./db/chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ec1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "# --- SQLite ---\n",
    "conn = sqlite3.connect(\"./db/sqlite/isVectorized.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vectorized_files (\n",
    "    file_id TEXT PRIMARY KEY,\n",
    "    hash TEXT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def is_vectorized(file_id, file_hash) -> bool:\n",
    "    cur.execute(\n",
    "        \"SELECT 1 FROM vectorized_files WHERE file_id=? AND hash=?\",\n",
    "        (file_id, file_hash)\n",
    "    )\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "\n",
    "# --- Chroma ---\n",
    "client = chromadb.PersistentClient(path=\"./db/chroma/\")\n",
    "collection = client.get_or_create_collection(\"ALL_TEXT_FILES\")\n",
    "\n",
    "\n",
    "def clean_content(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def chunking(text: str, chunk_size: int, overlap: int) -> list[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        if end == text_length:\n",
    "            break\n",
    "\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def add_to_chroma(file_path: str):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    content = clean_content(raw)\n",
    "    file_hash = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    if is_vectorized(file_path, file_hash):\n",
    "        #tqdm.write(f\"[DEV] Skipping AV:{file_path[50:70]}\")\n",
    "        return\n",
    "\n",
    "    #tqdm.write(f\"[DEV] Attempting AV:{file_path[50:70]}\")\n",
    "\n",
    "    chunks = chunking(content, 1000, 200)\n",
    "\n",
    "    # 1ï¸âƒ£ Add to Chroma FIRST\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        collection.add(\n",
    "            documents=[chunk],\n",
    "            metadatas=[{\"file_id\": file_path}],\n",
    "            ids=[f\"{file_path}::chunk_{i}\"]\n",
    "        )\n",
    "\n",
    "    # 2ï¸âƒ£ Mark vectorized AFTER success\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO vectorized_files (file_id, hash)\n",
    "        VALUES (?, ?)\n",
    "        ON CONFLICT(file_id) DO UPDATE SET\n",
    "            hash = excluded.hash\n",
    "        \"\"\",\n",
    "        (file_path, file_hash)\n",
    "    )\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31752f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_chroma(files_to_vectorize[12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85f1bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['C:\\\\Users\\\\Jamin Carter\\\\Downloads\\\\web_archive\\\\2025-12-18__github_com_open_webui_open_webui.txt::chunk_0'], 'embeddings': None, 'documents': [\"Skip to content\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\nDismiss alert\\nopen-webui\\nPublic\\nSponsor\\nWatch\\nCouldn't load subscription status.\\nRetry\\nUh oh!\\nThere was an error while loading. Please reload this page.\\nFork\\n16.6k\\nFork your own copy of open-webui/open-webui\\nForks could not be loaded\\nLoading\\nUh oh!\\nThere was an error while loading. Please reload this page.\\nStarred\\n118k\\nLoading\\nUh oh!\\nThere was an error while loading. Please reload this page.\\nStar\\n118k\\nLoading\\nUh oh!\\nThere was an error while loading. Please reload this page.\\nWatch536\\nSponsor open-webui/open-webui\\nSponsor open-webui/open-webui\\nGitHub Sponsors\\nLearn more about Sponsors\\ntjbck\\ntjbck\\nSponsor\\nLearn more about funding links in repositories.\\nReport abuse\\nFork your own copy of open-webui/open-webui\\nUnstar this repository\\nLoading\\nUh oh!\\nTher\"], 'uris': None, 'included': ['documents', 'metadatas'], 'data': None, 'metadatas': [{'file_id': 'C:\\\\Users\\\\Jamin Carter\\\\Downloads\\\\web_archive\\\\2025-12-18__github_com_open_webui_open_webui.txt'}]}\n"
     ]
    }
   ],
   "source": [
    "result = collection.get(\n",
    "    limit=1,\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4565be1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 861.83it/s, file=]                                                                          \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(files_to_vectorize, desc=\"Vectorizing files\")\n",
    "\n",
    "for file_path in pbar:\n",
    "    pbar.set_postfix(file=file_path[56:])  # show last part only\n",
    "    add_to_chroma(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ede7069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"traction engines (Tika, Docling, Document Intelligence, Mistral OCR, External loaders). Load documents directly into chat or add files to your document library, effortlessly accessing them using the # command before a query.\\nðŸ” Web Search for RAG: Perform web searches using 15+ providers including SearXNG, Google PSE, Brave Search, Kagi, Mojeek, Tavily, Perplexity, serpstack, serper, Serply, DuckDuckGo, SearchApi, SerpApi, Bing, Jina, Exa, Sougou, Azure AI Search, and Ollama Cloud, injecting results directly into your chat experience.\\nðŸŒ Web Browsing Capability: Seamlessly integrate websites into your chat experience using the # command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\\nðŸŽ¨ Image Generation & Editing Integration: Create and edit images using multiple engines including OpenAI's DALL-E, Gemini, ComfyUI (local), and AUTOMATIC1111 (local), with support for both generatio\",\n",
       "  'ommit detailsauto-archiverauto-archiverimplemented blackboxDec 18, 2025DEV.mdDEV.mdimplemented blackboxDec 18, 2025README.mdREADME.mdfirst commitDec 18, 2025main.ipynbmain.ipynbimplemented extensionDec 18, 2025View all filesDrop to upload your filesRepository files navigationEdit fileWebScraping\\nEdit repository details\\nDescription\\nWebsite\\nTopics (separate with spaces)\\nInclude in the home page\\nReleases\\nPackages\\nDeployments\\nSave changes\\nCancel\\nAbout\\nA python based website specific scraping tool to be used for downstream tasks such as domain based question answer. Mainly the puropse of tis project is that you give it a website, and a depth and it scraps all data and store it to a database for RAG by llms.\\nResources\\nReadme\\nUh oh!\\nThere was an error while loading. Please reload this page.\\nActivity\\nStars\\n0\\nstars\\nWatchers\\n0\\nwatching\\nForks\\n0\\nforks\\nReleases\\nNo releases published\\nCreate a new release\\nPackages\\n0\\nNo packages published\\nPublish your first package\\nLanguages\\nJupyter Notebook\\n59.4%\\nJav',\n",
       "  \"e by @ThePrimeagen\\nBoot dev and The PrimeTime\\nBoot dev and The PrimeTime\\nBoot dev and The PrimeTime\\nâ€¢\\nâ€¢\\n360k views\\n4 months ago\\nBoot dev and The PrimeTime\\nBoot dev and The PrimeTime\\nBoot dev and The PrimeTime\\nThe web is built on HTTP, and there's no better way to understand how something works than to implement it yourself. In this\\xa0...\\nThe web is built on HTTP, and there's no better way to understand how something works than to implement it yourself. In this\\xa0...\\nFrom the video description\\n11 chapters\\nIntroduction To The Course | Chapter 1 - HTTP Streams | Chapter 2 - TCP | Chapter 3 - Requests | Chapter 4 - Request Lines | Chapter 5 - HTTP Headers | Chapter 6 - HTTP Body | Chapter 7 - HTTP Responses | Chapter 8 - Chunked Encoding | Chapter 9 - Binary Data | Outro\\n11 chapters in this video\\n11 chapters\\nIntroduction To The Course\\nIntroduction To The Course\\n0:00\\nIntroduction To The Course\\n0:00\\nChapter 1 - HTTP Streams\\nChapter 1 - HTTP Streams\\n4:08\\nChapter 1 - HTTP Streams\\n4:08\\nChapter 2 - \"]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_chroma(query: str, n_results: int = 5):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    return results\n",
    "\n",
    "query_chroma(\"What is web scraping?\", n_results=3)[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58796841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listener active. Press Ctrl+Alt+V to search ChromaDB...\n",
      "Done! Prompt for 'What is the best wea...' is in your clipboard.\n"
     ]
    }
   ],
   "source": [
    "import keyboard\n",
    "import pyautogui\n",
    "import pyperclip\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "# 1. Use the query function you already built in Cell 9\n",
    "# (Ensure you've run Cell 9 before starting the listener)\n",
    "\n",
    "SYSTEMPROMPT = \"\"\"\n",
    "### SYSTEM INSTRUCTIONS\n",
    "You are a helpful AI assistant. Your primary task is to answer the user's query using the PROVIDED CONTEXT below. \n",
    "1. Always start your response by acknowledging the context (e.g., \"According to the provided documents...\").\n",
    "2. If the answer is not in the context, explicitly state: \"The provided context does not contain information about this, but based on general knowledge...\"\n",
    "3. Keep your response concise and directly related to the user's query.\n",
    "4. If context is relevant but insufficient, combine it with your own knowledge to provide a comprehensive answer. Explicitly mention that the answer is based on your knowledge.\n",
    "\n",
    "You will be provided with a Query and a set of documents from a vector database as context.\n",
    "\"\"\"\n",
    "\n",
    "def get_selected_text():\n",
    "    \"\"\"Tries to grab highlighted text from any window.\"\"\"\n",
    "    old_clipboard = pyperclip.paste()\n",
    "    pyperclip.copy('') \n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    time.sleep(0.15) \n",
    "    \n",
    "    selected_text = pyperclip.paste().strip()\n",
    "    if not selected_text:\n",
    "        pyperclip.copy(old_clipboard)\n",
    "        return None\n",
    "    return selected_text\n",
    "\n",
    "def trigger_ai_search():\n",
    "    \"\"\"Main logic that runs when you press the shortcut.\"\"\"\n",
    "    # Try to grab selection first\n",
    "    user_query = get_selected_text()\n",
    "\n",
    "    # Fallback to popup if nothing highlighted\n",
    "    if not user_query:\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        user_query = simpledialog.askstring(\"Query Chroma\", \"Enter your query:\")\n",
    "        root.destroy()\n",
    "    \n",
    "    if user_query:\n",
    "        # IMPORTANT: We MUST call the query function here to get FRESH results\n",
    "        # search_data will look like: {'documents': [[doc1, doc2]], ...}\n",
    "        search_data = query_chroma(user_query, n_results=3) \n",
    "        \n",
    "        formatted_response = SYSTEMPROMPT + f\"\\n### CONTEXT FROM VECTOR DB\\nQuery: {user_query}\\n\\n\"\n",
    "        \n",
    "        # FIX: We iterate over search_data['documents'][0]\n",
    "        # This is a list of actual text blocks, not characters.\n",
    "        for doc_text in search_data['documents'][0]:\n",
    "            formatted_response += f\"- {doc_text}\\n\\n\"\n",
    "            \n",
    "        formatted_response += \"--- END OF CONTEXT ---\"\n",
    "\n",
    "        pyperclip.copy(formatted_response)\n",
    "        print(f\"Done! Prompt for '{user_query[:20]}...' is in your clipboard.\")\n",
    "\n",
    "# Register the shortcut\n",
    "keyboard.add_hotkey('ctrl+alt+v', trigger_ai_search)\n",
    "\n",
    "print(\"Listener active. Press Ctrl+Alt+V to search ChromaDB...\")\n",
    "keyboard.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d239dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
